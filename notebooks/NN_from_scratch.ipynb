{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"Gadfly\")\n",
    "using Gadfly\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@enum DataSource concentriccircles=1 spiral=2\n",
    "\n",
    "samples_per_class = 100; # number of data points per class\n",
    "data_distortion = 0.02; # set to 0 for no distortion\n",
    "data_source = spiral; # spiral or circle\n",
    "num_classes = 3; # number of different classes\n",
    "\n",
    "const num_dimensions = 2; # Number of dimensions (x, y), changing this will break all plotting\n",
    "\n",
    "# computed values\n",
    "N = samples_per_class * num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate our data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "function getCircle(dots, radius, x=0, y=0)\n",
    "    angles = randn(dots).*(2*pi)\n",
    "    xs = cos(angles).*radius.+x\n",
    "    ys = sin(angles).*radius.+y\n",
    "    return [ xs ys ]\n",
    "end\n",
    "\n",
    "function getSpiral(dots, degrees, rate=1.0)\n",
    "    angles = linspace(pi/3, degrees, dots)\n",
    "    radii = angles.*rate\n",
    "    X = [ f(angles[i]) * radii[i] for i in 1:dots, f in [cos, sin]]\n",
    "end\n",
    "\n",
    "if data_source == concentriccircles\n",
    "    Data = getCircle(samples_per_class, 1.5)\n",
    "    Labels = ones(Int64, samples_per_class)\n",
    "    for i in 2:num_classes\n",
    "        Data = vcat(Data, getCircle(samples_per_class, i*1.5, 2, 2))\n",
    "        Labels = vcat(Labels, ones(Int64, samples_per_class).*i)\n",
    "    end\n",
    "elseif data_source == spiral\n",
    "    Data = getSpiral(samples_per_class, 2pi, 1.5)\n",
    "    Labels = ones(Int64, samples_per_class)\n",
    "    for i in 2:num_classes\n",
    "        Data = vcat(Data, getSpiral(samples_per_class, 2pi, i*1.5))\n",
    "        Labels = vcat(Labels, ones(Int64, samples_per_class).*i)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helper functions\n",
    "function predictClasses(data, getScores)\n",
    "    scores = getScores(data)\n",
    "    exp_scores = e.^scores\n",
    "    _, max_indices = findmax(exp_scores', 1)\n",
    "    transpose((max_indices-1)%num_classes+1)\n",
    "end\n",
    "\n",
    "function output_learning_status_image(iteration, getScores)\n",
    "    data = [ [x y] for x in linspace(-1, 1, 50), y in linspace(-1, 1, 50) ]\n",
    "    data = vcat(data...)\n",
    "    data = convert(DataFrame, [data predictClasses(data, getScores)])\n",
    "    original_data = convert(DataFrame, [DistortedData Labels])\n",
    "    p = plot(\n",
    "        layer(original_data, x=:x1, y=:x2, color=:x3, Geom.point),\n",
    "        layer(data, x=:x1, y=:x2, color=:x3, Geom.point),\n",
    "        Guide.title(\"Iteration $(iteration)\")\n",
    "    )\n",
    "    draw(SVGJS(\"./images/test_$(lpad(iteration,6,0)).js.svg\", 6inch, 6inch), p)\n",
    "end\n",
    "\n",
    "function plot_data(df)\n",
    "    plot(df, x=:x1, y=:x2, color=:x3, Geom.point)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data so that it is between -1 and 1\n",
    "NormalizedData = Data ./ repmat(maximum(abs(Data), 1), N) \n",
    "plot_data(convert(DataFrame, [NormalizedData Labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some randomization to the dataset\n",
    "DistortedData = NormalizedData + randn(N, num_dimensions).*data_distortion;\n",
    "plot_data(convert(DataFrame, [DistortedData Labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic linear regression implementation\n",
    "function linear_regression(data, labels)\n",
    "    \n",
    "    # parameters\n",
    "    step_size = 1e-1;\n",
    "    iterations = 1000;\n",
    "    regularization = 1e-3; # regularization strength\n",
    "    \n",
    "    # randomly initialize weights\n",
    "    W = randn(num_dimensions, num_classes) .* 0.01\n",
    "\n",
    "    # randomly initialize bias vector\n",
    "    b = zeros(1, num_classes)\n",
    "    \n",
    "    # compute the gradient on normalized_scores (based on the derivative of the loss function)\n",
    "    correct_class_bitmask = zeros(N, num_classes)\n",
    "    for (index, label_idx) in enumerate(labels)\n",
    "        correct_class_bitmask[index, Int64(label_idx)] = 1\n",
    "    end\n",
    "\n",
    "    # begine linear regression\n",
    "    for iter in 1:iterations\n",
    "\n",
    "        # compute scores\n",
    "        scores = data * W + repmat(b, N)\n",
    "\n",
    "        # compute normalized log probabilities (e^scores)\n",
    "        exp_scores = exp(scores)\n",
    "\n",
    "        # e^scores / ( sum(e^scores) )\n",
    "        normalized_scores = exp_scores ./ repmat(sum(exp_scores, 2)', size(exp_scores, 2))'\n",
    "\n",
    "        # normalized log-likelyhood -log(e^scores / ( sum(e^scores)))\n",
    "        normalized_log_likelyhood = -log(normalized_scores[map(b -> b == 1 ,correct_class_bitmask)])\n",
    "        total_error = sum(normalized_log_likelyhood)/N + 0.5*regularization*sum(W.*W)\n",
    "        if iter < 10 || iter % 100 == 0\n",
    "            println(\"ITER: $(iter), ERROR: $(total_error)\")\n",
    "            output_learning_status_image(iter, (d) -> d * W + repmat(b, size(d, 1)))\n",
    "        end\n",
    "        \n",
    "        # log likelihood gradient\n",
    "        dscores = normalized_scores - correct_class_bitmask\n",
    "        dscores ./= N\n",
    "        \n",
    "        dW = data'*dscores + regularization.*W \n",
    "        db = sum(dscores, 1)\n",
    "        \n",
    "        W += -step_size .* dW\n",
    "        b += -step_size * db\n",
    "    end\n",
    "    return W, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_regression(DistortedData, Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function linear_regression_two_layers(data, labels)\n",
    "    step_size = 1e-1\n",
    "    iterations = 10000\n",
    "    hidden_layer_size = 100\n",
    "    regularization = 1e-3; # regularization strength\n",
    "        \n",
    "    # randomly initialize weights\n",
    "    W = randn(num_dimensions, hidden_layer_size) .* 0.01\n",
    "    W2 = randn(hidden_layer_size, num_classes).* 0.01\n",
    "    \n",
    "    # randomly initialize bias vector\n",
    "    b = zeros(1, hidden_layer_size)\n",
    "    b2 = zeros(1, num_classes)\n",
    "    \n",
    "    # compute the gradient on normalized_scores\n",
    "    correct_class_bitmask = zeros(N, num_classes)\n",
    "    for (index, label_idx) in enumerate(labels)\n",
    "        correct_class_bitmask[index, Int64(label_idx)] = 1\n",
    "    end\n",
    "\n",
    "    # begine linear regression\n",
    "    for iter in 1:iterations\n",
    "\n",
    "        # compute scores (forward pass)\n",
    "        hidden_layer = max(0, data * W + repmat(b, N))\n",
    "        scores = hidden_layer * W2 + repmat(b2, N)\n",
    "\n",
    "        # compute normalized log probabilities (e^scores)\n",
    "        exp_scores = e.^scores\n",
    "\n",
    "        # e^scores / ( sum(e^scores) )\n",
    "        normalized_scores = exp_scores ./ repmat(sum(exp_scores, 2)', size(exp_scores, 2))'\n",
    "\n",
    "        # normalized log-likelyhood -log(e^scores / ( sum(e^scores)))\n",
    "        normalized_log_likelyhood = -log(normalized_scores[map(b -> b == 1, correct_class_bitmask)])\n",
    "        total_error = sum(normalized_log_likelyhood)/N \n",
    "            + 0.5*regularization*sum(W.*W) \n",
    "            + 0.5*regularization*sum(W2.*W2)\n",
    "        if (iter < 100 && iter % 10 == 0) || iter % 1000 == 0\n",
    "            println(\"ITER: $(iter), ERROR: $(total_error)\")\n",
    "            output_learning_status_image(\n",
    "                iter, \n",
    "                (d) -> max(0, d*W+repmat(b, size(d, 1)))*W2+repmat(b2, size(d, 1))\n",
    "            )\n",
    "        end\n",
    "            \n",
    "        # log likelihood gradient\n",
    "        dscores = normalized_scores - correct_class_bitmask\n",
    "        dscores ./= N\n",
    "        \n",
    "        # backpropate the gradient to the parameters\n",
    "        # first backprop into parameters W2 and b2\n",
    "        dW2 = hidden_layer'*dscores + regularization .* W2\n",
    "        db2 = sum(dscores, 1)\n",
    "        \n",
    "        # next backprop into hidden layer\n",
    "        dhidden = dscores*W2'\n",
    "        \n",
    "        # backprop the ReLU non-linearity\n",
    "        dhidden[hidden_layer .<= 0] = 0\n",
    "\n",
    "        # finally into W,b\n",
    "        dW = data' * dhidden + regularization .* W\n",
    "        db = sum(dhidden, 1)\n",
    "        \n",
    "        # update our parameters\n",
    "        W += -step_size .* dW\n",
    "        b += -step_size * db\n",
    "        W2 += -step_size .* dW2\n",
    "        b2 += -step_size * db2\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_regression_two_layers(DistortedData, Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
